local vertex = require("vertex_client")
local output = require("output")
local prompt_mapper = require("prompt_mapper")

-- Create module table
local text_handler = {}

-- Main handler function
function text_handler.handler(args)
    -- Validate required arguments
    if not args or not args.model then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "Model is required in arguments"
        }
    end

    -- Initialize messages safely
    local messages_internal = args.messages or {}
    if #messages_internal == 0 then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "No messages provided"
        }
    end

    -- Map messages to Vertex AI format using the prompt mapper
    -- This now returns TWO values
    local contents, system_instruction = prompt_mapper.map_to_vertex(messages_internal)

    -- Configure options objects for easier management
    local options = args.options or {}

    -- Configure request payload
    local payload = {
        contents = contents,
        -- systemInstruction added below if present
        generationConfig = {
            -- Include common options, filter out nil values later
            stopSequences = options.stop_sequences,
            maxOutputTokens = options.max_tokens,
            temperature = options.temperature,
            topP = options.top_p,
            seed = options.seed,
            presencePenalty = options.presence_penalty,
            -- response_mime_type = "text/plain" -- Usually not needed for default text, omit unless required
        }
    }

    -- Add system instruction if generated by the mapper
    if system_instruction then
        payload.systemInstruction = system_instruction
    end

    -- Clean up nil values from generationConfig
    local cleaned_gen_config = {}
    for key, value in pairs(payload.generationConfig) do
        if value ~= nil then
            cleaned_gen_config[key] = value
        end
    end
    -- Only include generationConfig if it's not empty
    if next(cleaned_gen_config) then
        payload.generationConfig = cleaned_gen_config
    else
        payload.generationConfig = nil
    end


    -- Make the request
    local request_options = {
        timeout = args.timeout or 120,
        base_url = args.endpoint, -- Allow overriding base URL
        location = args.location or nil,
        project = args.project or nil
    }

    local response, err = vertex.request(
        vertex.DEFAULT_GENERATE_CONTENT_ENDPOINT,
        args.model,
        payload,
        request_options
    )

    -- Handle client/request errors using the centralized mapper
    if err then
        if vertex.map_error then
            return vertex.map_error(err)
        else
            return { error = output.ERROR_TYPE.SERVER_ERROR, error_message = err.message or "Vertex client error" }
        end
    end

    -- Check response validity and structure
    if not response or not response.candidates or #response.candidates == 0 then
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "Invalid or empty response structure from Vertex AI"
        }
    end

    -- Process the first candidate
    local first_candidate = response.candidates[1]
    local content_text = ""
    local finish_reason_key = "UNKNOWN"

    if first_candidate.finishReason then
        finish_reason_key = first_candidate.finishReason
    end

    -- Check for non-OK finish reasons first
    local mapped_finish_reason = vertex.FINISH_REASON_MAP[finish_reason_key] or output.FINISH_REASON.UNKNOWN
    if mapped_finish_reason == output.FINISH_REASON.CONTENT_FILTER then
        return {
            error = output.ERROR_TYPE.CONTENT_FILTER,
            error_message = "Response blocked due to safety filters (Reason: " .. finish_reason_key .. ")",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    elseif mapped_finish_reason == output.FINISH_REASON.ERROR then
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "Vertex AI reported an error (Reason: " .. finish_reason_key .. ")",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    end

    -- Extract content text from parts
    if first_candidate.content and first_candidate.content.parts then
        for _, content_part in ipairs(first_candidate.content.parts) do
            if content_part.text then
                content_text = content_text .. content_part.text
            end
        end
    else
        -- If finish reason was STOP or MAX_TOKENS but no content parts, it's still an issue
        if mapped_finish_reason == output.FINISH_REASON.STOP or mapped_finish_reason == output.FINISH_REASON.LENGTH then
            return {
                error = output.ERROR_TYPE.SERVER_ERROR,
                error_message = "Invalid candidate content structure (no parts) despite successful finish reason (" ..
                finish_reason_key .. ")"
            }
            -- Otherwise handled by finish reason check above
        end
    end

    -- Check if content was actually extracted (could be empty if finishReason was e.g. SAFETY)
    -- We already checked for SAFETY etc., so if content is empty now but finish reason was STOP/MAX_TOKENS, it's odd.
    if content_text == "" and (mapped_finish_reason == output.FINISH_REASON.STOP or mapped_finish_reason == output.FINISH_REASON.LENGTH) then
        -- Model might genuinely produce empty string, but let's log a warning potentially?
        -- Or return normally with empty string result. Let's return normally.
    elseif content_text == "" and mapped_finish_reason == output.FINISH_REASON.UNKNOWN then
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "No text content found in Vertex AI response candidate and finish reason unknown.",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    end


    -- Extract token usage information
    local tokens = nil
    if response.usageMetadata then
        tokens = output.usage(
            response.usageMetadata.promptTokenCount or 0,
            response.usageMetadata.candidatesTokenCount or 0,
            response.usageMetadata.thoughtsTokenCount or 0,
            0, -- cache_write_tokens
            0  -- cache_read_tokens
        )
    end

    -- Return successful response
    return {
        result = content_text,              -- The concatenated text string
        tokens = tokens,
        metadata = response.metadata or {}, -- Ensure metadata exists
        finish_reason = mapped_finish_reason,
        provider = "vertex",
        model = args.model
    }
end

-- Return the module table containing the handler
return text_handler
