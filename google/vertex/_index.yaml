version: "1.0"
namespace: wippy.llm.google.vertex

entries:
  # wippy.llm.google.vertex:client
  - name: client
    kind: library.lua
    meta:
      comment: Vertex AI API client library with error handling and response formatting. For internal use only, use LLM instead.
      depends_on:
        - ns:wippy.llm
      provider: vertex
    source: file://client.lua
    modules:
      - http_client
      - json
      - env
      - store
      - ctx
    imports:
      output: wippy.llm:output
    
  # wippy.llm.google.vertex:prompt_mapper
  - name: prompt_mapper
    kind: library.lua
    meta:
      comment: Maps internal prompt format to Vertex-compatible message format
      depends_on:
        - ns:wippy.llm
      provider: vertex
    source: file://prompt_mapper.lua
    modules:
      - json
    imports:
      prompt: wippy.llm:prompt
    
  # wippy.llm.google.vertex:structured_output
  - name: structured_output
    kind: function.lua
    meta:
      type: llm_function
      comment: Vertex AI structured output implementation with schema enforcement
      depends_on:
        - ns:wippy.llm
      priority: 40
      provider: vertex
      supports:
        - schema
    source: file://structured_output.lua
    modules:
      - json
      - hash
    imports:
      output: wippy.llm:output
      prompt_mapper: wippy.llm.google.vertex:prompt_mapper
      vertex_client: wippy.llm.google.vertex:client
    method: handler
    pool:
      max_size: 25
    
  # wippy.llm.google.vertex:structured_output_test
  - name: structured_output_test
    kind: function.lua
    meta:
      name: Vertex AI Structured Output Test
      type: test
      comment: Tests the Vertex AI structured output handler functionality
      group: Vertex AI Library
      tags:
        - llm
        - vertex
        - schema
        - structured_output
      depends_on:
        - ns:wippy.llm
      provider: vertex
    source: file://structured_output_test.lua
    modules:
      - json
      - env
    imports:
      prompt: wippy.llm:prompt
      output: wippy.llm:output
      structured_output: wippy.llm.google.vertex:structured_output
      test: wippy.test:test
      vertex_client: wippy.llm.google.vertex:client
    method: run_tests
    
  # wippy.llm.google.vertex:text_generation
  - name: text_generation
    kind: function.lua
    meta:
      type: llm_function
      comment: Vertex AI text generation implementation
      depends_on:
        - ns:wippy.llm
      priority: 100
      provider: vertex
      supports:
        - generate
    source: file://text_generation.lua
    modules:
      - json
      - env
      - security
    imports:
      output: wippy.llm:output
      prompt_mapper: wippy.llm.google.vertex:prompt_mapper
      vertex_client: wippy.llm.google.vertex:client
    method: handler
    pool:
      max_size: 25
    
  # wippy.llm.google.vertex:text_generation_test
  - name: text_generation_test
    kind: function.lua
    meta:
      name: Vertex AI Text Generation Test
      type: test
      comment: Tests the Vertex AI text generation handler functionality
      group: Vertex AI Library
      tags:
        - llm
        - vertex
        - generation
      depends_on:
        - ns:wippy.llm
      provider: vertex
    source: file://text_generation_test.lua
    modules:
      - json
      - env
    imports:
      prompt: wippy.llm:prompt
      output: wippy.llm:output
      test: wippy.test:test
      text_generation: wippy.llm.google.vertex:text_generation
      vertex_client: wippy.llm.google.vertex:client
    method: run_tests
    
  # wippy.llm.google.vertex:token_refresh
  - name: token_refresh
    kind: process.lua
    meta:
      comment: Process that periodically refreshes tokens
      depends_on:
        - ns:wippy.llm
      default_host: app:processes
      provider: vertex
    source: file://token_refresh.lua
    modules:
      - time
      - json
      - base64
      - crypto
      - http_client
      - env
      - store
    method: run
    
  # wippy.llm.google.vertex:token_refresh.service
  - name: token_refresh.service
    kind: process.service
    meta:
      comment: Token Refresh Service
      depends_on:
        - ns:wippy.llm
      provider: vertex
    host: app:processes
    lifecycle:
      depends_on:
        - app:processes
      auto_start: true
      security:
        groups:
          - wippy.security:process
    process: token_refresh
    
  # wippy.llm.google.vertex:tool_calling
  - name: tool_calling
    kind: function.lua
    meta:
      type: llm_function
      comment: Vertex AI function/tool calling implementation
      depends_on:
        - ns:wippy.llm
      priority: 50
      provider: vertex
      supports:
        - generate
        - tools
    source: file://tool_calling.lua
    imports:
      tools: wippy.llm:tools
      output: wippy.llm:output
      prompt_mapper: wippy.llm.google.vertex:prompt_mapper
      vertex_client: wippy.llm.google.vertex:client
    method: handler
    pool:
      max_size: 25
    
  # wippy.llm.google.vertex:tool_calling_test
  - name: tool_calling_test
    kind: function.lua
    meta:
      name: Vertex AI Tool Calling Test
      type: test
      comment: Tests the Vertex AI tool/function calling handler functionality
      group: Vertex AI Library
      tags:
        - llm
        - vertex
        - tools
        - function_calling
      depends_on:
        - ns:wippy.llm
      provider: vertex
    source: file://tool_calling_test.lua
    modules:
      - json
      - env
    imports:
      prompt: wippy.llm:prompt
      tools: wippy.llm:tools
      output: wippy.llm:output
      test: wippy.test:test
      tool_calling: wippy.llm.google.vertex:tool_calling
      vertex_client: wippy.llm.google.vertex:client
    method: run_tests
    