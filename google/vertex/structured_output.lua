local json = require("json")
local vertex = require("vertex_client")
local output = require("output")
local prompt_mapper = require("prompt_mapper")

-- Create module table for exports
local structured_output = {}

-- Basic validation (can be expanded based on Vertex limits)
function structured_output.validate_schema(schema)
    local errors = {}
    if not schema or type(schema) ~= "table" then
        return false, { "Schema must be a table" }
    end
    -- Vertex requires the root to be an object for schema enforcement.
    if schema.type ~= "object" then
        table.insert(errors, "Root schema type must be 'object'")
    end
    -- While not strictly enforced by the API for *all* models yet,
    -- requiring no additional properties is good practice for predictable output.
    -- if schema.additionalProperties == true then
    --    table.insert(errors, "Root schema should ideally have additionalProperties: false (or omitted)")
    -- end

    -- Add more checks here if needed (depth, property count, etc.)

    return #errors == 0, errors
end

-- Main handler for Vertex AI Structured Output
function structured_output.handler(args)
    -- Validate required arguments
    if not args or not args.model then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "Model is required in arguments"
        }
    end
    if not args.schema then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "Schema is required for structured output"
        }
    end

    -- Validate the schema
    local schema_valid, schema_errors = structured_output.validate_schema(args.schema)
    if not schema_valid then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "Invalid schema: " .. table.concat(schema_errors, "; ")
        }
    end

    -- Initialize messages safely
    local messages_internal = args.messages or {}
    if #messages_internal == 0 then
        return {
            error = output.ERROR_TYPE.INVALID_REQUEST,
            error_message = "No messages provided"
        }
    end

    -- Map messages to Vertex AI format using the prompt mapper
    -- This now returns TWO values
    local contents, system_instruction = prompt_mapper.map_to_vertex(messages_internal)

    -- Configure options objects for easier management
    local options = args.options or {}

    -- Configure request payload
    local payload = {
        contents = contents,
        -- systemInstruction added below if present
        generationConfig = {
            -- Include common options, filter out nil values later
            stopSequences = options.stop_sequences,
            maxOutputTokens = options.max_tokens,
            temperature = options.temperature,
            topP = options.top_p,
            seed = options.seed,
            presencePenalty = options.presence_penalty,
            -- Specific for structured output:
            response_mime_type = "application/json", -- MUST be application/json
            responseSchema = args.schema             -- Use the provided schema
        }
    }

    -- Add system instruction if generated by the mapper
    if system_instruction then
        payload.systemInstruction = system_instruction
    end

    -- Clean up nil values from generationConfig
    local cleaned_gen_config = {}
    for key, value in pairs(payload.generationConfig) do
        if value ~= nil then
            cleaned_gen_config[key] = value
        end
    end
    -- Only include generationConfig if it's not empty
    if next(cleaned_gen_config) then
        payload.generationConfig = cleaned_gen_config
    else
        payload.generationConfig = nil
    end

    -- Make the request
    local request_options = {
        timeout = args.timeout or 120,
        base_url = args.endpoint, -- Allow overriding base URL
        location = args.location or nil,
        project = args.project or nil
    }

    local response, err = vertex.request(
        vertex.DEFAULT_GENERATE_CONTENT_ENDPOINT,
        args.model,
        payload,
        request_options
    )

    -- Handle client/request errors using the centralized mapper
    if err then
        -- Ensure vertex_client exports map_error function
        if vertex.map_error then
            return vertex.map_error(err)
        else
            -- Basic fallback error handling
            return { error = output.ERROR_TYPE.SERVER_ERROR, error_message = err.message or "Vertex client error" }
        end
    end

    -- Check response validity and structure
    if not response or not response.candidates or #response.candidates == 0 then
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "Invalid or empty response structure from Vertex AI"
        }
    end

    -- Process the first candidate (structured output usually has one)
    local first_candidate = response.candidates[1]
    local content_text = ""
    local finish_reason_key = "UNKNOWN"

    if first_candidate.finishReason then
        finish_reason_key = first_candidate.finishReason
    end

    -- Check for non-OK finish reasons first
    local mapped_finish_reason = vertex.FINISH_REASON_MAP[finish_reason_key] or output.FINISH_REASON.UNKNOWN
    if mapped_finish_reason == output.FINISH_REASON.CONTENT_FILTER then
        return {
            error = output.ERROR_TYPE.CONTENT_FILTER,
            error_message = "Response blocked due to safety filters (Reason: " .. finish_reason_key .. ")",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    elseif mapped_finish_reason == output.FINISH_REASON.ERROR then
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "Vertex AI reported an error (Reason: " .. finish_reason_key .. ")",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    end

    -- Extract content text from parts
    if first_candidate.content and first_candidate.content.parts then
        for _, content_part in ipairs(first_candidate.content.parts) do
            -- The JSON object should be in the text field when response_mime_type is application/json
            if content_part.text then
                content_text = content_text .. content_part.text
            end
        end
    else
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "Invalid candidate content structure in Vertex AI response"
        }
    end

    -- Check if content was actually extracted
    if content_text == "" then
        -- It's possible the model finished for other reasons (like MAX_TOKENS) before producing valid JSON
        return {
            error = output.ERROR_TYPE.SERVER_ERROR,
            error_message = "No text content found in Vertex AI response candidate (Finish Reason: " ..
            finish_reason_key .. ")",
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    end

    -- Decode the JSON content string
    local parsed_content, decode_err = json.decode(content_text)
    local final_result = nil

    if decode_err then
        -- If decoding fails, it means the model didn't adhere to the schema or response format.
        -- Return an error indicating this, including the raw text for debugging.
        return {
            error = output.ERROR_TYPE.INVALID_RESPONSE, -- Use a specific error type if available
            error_message = "Failed to decode Vertex AI response as JSON: " .. decode_err,
            raw_content = content_text,                 -- Include the raw text
            finish_reason = mapped_finish_reason,
            provider = "vertex",
            model = args.model,
            metadata = response.metadata
        }
    else
        -- Successfully decoded JSON
        final_result = parsed_content
    end

    -- Extract token usage information
    local tokens = nil
    if response.usageMetadata then
        tokens = output.usage(
            response.usageMetadata.promptTokenCount or 0,
            response.usageMetadata.candidatesTokenCount or 0,
            response.usageMetadata.thoughtsTokenCount or 0,
            0, -- cache_write_tokens
            0 -- cache_read_tokens
        )
    end

    -- Return successful response
    return {
        result = final_result,              -- The parsed JSON object
        tokens = tokens,
        metadata = response.metadata or {}, -- Ensure metadata exists
        finish_reason = mapped_finish_reason,
        provider = "vertex",
        model = args.model
    }
end

-- Return the module table containing the handler
return structured_output
